\documentclass[a4paper]{report}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[francais]{babel}
\usepackage[top=1.5cm, left=2.5cm, bottom=1.5cm, right=2.5cm]{geometry}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{tikz}

\author{Théo Verhelst}
\title{Résumé du cours d'Algorithmique et Recherche Opérationelle\\
\emph{INFO-F310}}

\theoremstyle{definition}
\newtheorem*{definition}{Définition}
\newtheorem*{notation}{Notation}
\theoremstyle{remark}
\newtheorem*{note}{Note}
\newtheorem*{example}{Exemple}
\theoremstyle{plain}
\newtheorem{theorem}{Théorème}
\newtheorem{corollary}{Corollaire}

\begin{document}
\maketitle
\tableofcontents

\chapter{Introduction}
\paragraph{}
Parmi les nombreux domaines compris dans l'algorithmique et la recherche
opérationelle, dans ce cours seront abordés la \emph{programmation mathématique}
et les \emph{méthodes combinatoires dans les graphes}.

\chapter{Programmation mathématique}
\section{Définition de la programmation mathématique}
\paragraph{}
La programmation mathématique est une modélisation de problèmes (qui peuvent
provenir d'une large gamme de domaines) ainsi que leur résolution.

\begin{definition}
Un problème de programmation mathématique est défini
par un tuple \((z, G)\), où
\[z:E^n\to E\]
est appelée \emph{fonction économique} (ou encore \emph{fonction de coût}), et
où \[\star\in\{=,\ge,\le\},\;G=\{(g_j(x_1,\dots,x_n)\star b_j)\;\forall
j\in\{1,\dots,m\}\}\] sont appelées \emph{contraintes}.
\((x_1,\dots,x_n)\) sont les \emph{variables} du problème.
\end{definition}

\begin{notation}
On notera
\[g_j(x_1,\dots,x_n)\begin{Bmatrix}\le\\\ge\\=\end{Bmatrix}b_j\quad\forall
j\in\{1,\dots,m\}\]
\end{notation}

\begin{definition}
On classe les problèmes selon la nature de l'ensemble
\(E\):
\begin{itemize}
	\item \(E=\mathbb{R}\) correspond aux problèmes continus
	\item \(E=\mathbb{Z}\) correspond aux problèmes entiers
	\item \(E=\{1,0\}\) correspond aux problèmes booléens
\end{itemize}
Ces classes peuvent être mixées, si toutes les variables ou contraintes ne sont
pas définies dans le même ensemble.
\end{definition}

\begin{definition}
Résoudre un problème de programmation mathématique consiste à trouver les
valeurs \((x_1,\dots,x_n)\) qui maximisent ou minimisent le plus possible la
fonction économique \(z\), tout en satisfaisant toutes les contraintes \(g_j\).
\end{definition}

\begin{definition}
Pour une solution donnée \((x_1,\dots,x_n)\), on dit qu'une contrainte
d'indice \(j\) est saturée quand:
\[g_j(x_1,\dots,x_n)=b_j\]
Cette notion n'est intéressante que pour les contraintes à inégalités, et
représente le cas où une ressource est utilisée à son maximum.
\end{definition}

\begin{definition}
Quand les fonctions \(z\) et \(g_j\) sont linéaires en \(x_i\), alors le
problème est appelé problème de programmation linéaire.
\end{definition}

\section{Classification des problèmes de programmation linéaire}
\begin{tikzpicture}
	\node (n1) at (8,10) {Mixtes - entiers};
	\node (n2) at (6,8)  {Mixtes - booléens};
	\node (n3) at (8,6)  {Booléens};
	\node (n4) at (10,8) {Entiers};
	\node (n5) at (4,6)  {Continus};

	\foreach \from/\to in {n2/n1, n4/n1, n5/n2, n3/n2}
		\draw[->,thick] (\from) -- (\to);
	\draw[<->,thick] (n3) -- (n4);
\end{tikzpicture}

\paragraph{}On peut classer les problèmes de programmation linéaire selon la
nature de leur variable. Le sens des flèches dans le schéma indique qu'une
méthode permettant de résoudre le problème à la destination de la flèche permet
également de résoudre un problème à la base de la flèche. On peut donc en
conclure qu'un solveur de problème mixant variables entières et continues permet
de résoudre tout type de problème de programmation linéaire.
\begin{note}
Un problème à nombre entiers peut également être résolu par un
solveur booléens: on pourrait imaginer convertir tous les variables entières en
suites de variables booléennes grâce à la représentation binaire du nombre.
\end{note}

\section{Respect d'un nombre paramétrique de contraintes}
On peut étendre la définition de la programmation mathématique en permettant
de ne respecter qu'un nombre \(m'\) de contraintes, avec \(m'<m\).
Pour cela, introduisons \(m\) variables booléennes \(\delta_i\) qui indiqueront
si la contrainte \(i\) est respectée. Introduisons également un nombre \(M\),
qui est supérieur à toutes les valeurs que peuvent prendre les contraintes
\(g_i\). On peut alors réécrire les contraintes
comme suit:
\[g_i(x_1,\dots,x_n)-b_i\begin{Bmatrix}\le\\\ge\\=\end{Bmatrix}M(1-\delta_i)\]
et rajouter la contrainte suivante:
\[\sum_{i=1}^{m}\delta_i\ge m'\]
Le problème résultant reste un problème de programmation linéaire si \(f\) et
\(g\) sont des fonctions linéaires.

\section{Problèmes  de programmation linéaire continus}
\subsection{Forme matricielle}
On commencera par exprimer les problèmes de programmation linéaire continus sous
forme matricielle:
\[\begin{cases}
	\text{Min\;} cx & c\in\mathbb{R}^{1\times n},\, x\in\mathbb{R}^{n\times 1}_+
	\\
	Ax \le b        & A\in\mathbb{R}^{m\times n},\, b\in\mathbb{R}^{m\times 1}
	\\
	x_i \ge 0       & \forall i\in\{1,\dots,n\}
\end{cases}\]
où \(x=[x_1,\dots,x_n]^T\) est le vecteur de variables, \(c\) est le vecteur de coefficients de
la fonction économique \(z\), \(A\) est la matrices de coefficients des
contraintes, et \(b=[b_1,\dots,b_m]^T\) est le vecteur de termes indépendants des contraintes.
Un certain nombre de restrictions sont imposées sur la formulation du problème,
car on peut toujours se ramener à ce problème plus restreint:
\begin{itemize}
	\item On se passe des contraintes en \(\ge\) et \(=\), car on peut
	toujours reformuler ces contraintes avec d'autres contraintes en \(\le\):
	\[\alpha=\beta\Leftrightarrow \alpha\le\beta\land-\alpha\le-\beta\]
	\[\alpha\ge\beta\Leftrightarrow-\alpha\le-\beta\]
	\[\forall\alpha,\beta\in\mathbb{R}\]

	\item On ne considère que les  problèmes où les variables
	\((x_1,\dots,x_n)\) sont positives, car ces variables
	représentent souvent des quantités, et ne peuvent donc pas être négatives.
	Si toutefois une variable \(x_i,\,i\in\{1,\dots,n\}\) peut être négative, on se
	ramène dans le cas positif en posant
	\[x_i=y_i-z_i\]
	où \(y_i,z_i\) sont deux nouvelles variables dans \(\mathbb{R}^+\).

	\item On ne considère que la minimisation de la fonction
	économique, car on peut ramener un problème de maximisation en un
	problème de minimisation en prenant l'opposé de la fonction économique.
\end{itemize}

\begin{notation}
	\[x\ge 0 := x_i\ge 0\;\forall i\in\{1,\dots,n\}\]
\end{notation}

\subsection{Variables d'écart}
Étant donné un problème continu linéaire
\[\begin{cases}
\text{Min\;} cx \\
Ax \le b \\
x \ge 0
\end{cases}\]
Afin de pouvoir résoudre le problème avec les outils de l'algèbre linéaire, on
introduit \(m\) nouvelles variables positives
\([t_1,\dots,t_m]=t\), et on reformule les contraintes comme suit:
\[Ax+t=b\]
Les variables \(t_j\) sont appelées variables d'écart, et représentent la
quantité de ressource qui est encore disponible pour une contrainte donnée.
On en déduit que quand \(t_j=0\), la contrainte \(j\) est saturée.
Par la suite, nous admettrons l'utilisation implicite de variables d'écart,
et considererons généralement les problèmes de la forme
\[\begin{cases}
\text{Min\;} cx \\
Ax = b \\
x \ge 0
\end{cases}\]

\begin{notation}
On notera le problème
\[\begin{cases}
\text{Min } cx \\
Ax \begin{Bmatrix}\le\\=\end{Bmatrix}b \\
x \ge 0
\end{cases}\]
comme suit:
\[\text{Min }\{cx:Ax\begin{Bmatrix}\le\\=\end{Bmatrix}b,x\ge0\}\]
\end{notation}

\subsection{Définitions}
\begin{definition}
Une \emph{solution} est une instance du vecteur \(x\) telle que \(Ax=b\).
\end{definition}

\begin{definition}
Une \emph{solution admissible} est une instance du
vecteur \(x\) telle que \(Ax=b\) et \(x \ge 0\).
\end{definition}

\begin{definition}
Une \emph{base} \(B\) est une matrice carrée \(m\times
m\) extraite de la matrice \(A\), avec \(\det(B)\ne 0\). On parlera
d'\emph{indices de base} (réciproquement \emph{hors base}) et de \emph{variables
de base} (réciproquement \emph{hors base}) quand ces indices ou variables sont
inclus (réciproquement exclus) dans la base \(B\). Les lignes et colonnes
incluses dans \(B\) ne doivent pas forcément être adjacentes dans \(A\).
\begin{example}
Soit la matrice
\[A=\begin{bmatrix}
	a_{11} & \dots  & a_{1m} & \dots  & a_{1n} \\
	\vdots & \ddots & \vdots & \ddots & \vdots \\
	a_{m1} & \dots  & a_{mm} & \dots  & a_{mn}
\end{bmatrix}\]
Une base \(B\) possible serait la matrice
\[B=\begin{bmatrix}
	a_{11} & \dots  & a_{1m} \\
	\vdots & \ddots & \vdots \\
	a_{m1} & \dots  & a_{mm}
\end{bmatrix}\]
Si et seulement si \(\det(B)\neq 0\). Les variables hors base sont alors
\((x_{m+1},\dots,x_{n})\).
\end{example}
\end{definition}

\begin{definition}
Une solution \(x\) est une \emph{solution de base}
associée à une base \(B\) si et seulement si les variables hors base sont
nulles.
\end{definition}

\begin{definition}
Une solution de base \(x\) est \emph{explicitée} si et
seulement si la base associée \(B\) est la matrice unité \(m\times m\).
\end{definition}

\begin{definition}
Une \emph{combinaison linéaire convexe} d'éléments \(p_1,\dots,p_n\) est
une expression de la forme \[\sum_{i=1}^n\alpha_ip_i\] où \(\alpha_i\in[0,1]
\;\forall i\in\{1,\dots,n\}\) et où \(\sum_{i=1}^n\alpha_i=1\). \\
Par exemple, pour deux points \(p_1\) et \(p_2\), cette expression représente le
segment de droite reliant \(p_1\) et \(p_2\).
\end{definition}

\begin{definition}
Un ensemble \(P\) est \emph{convexe} si est seulement si toute combinaison
linéaire convexe de ses éléments appartient également à \(P\):
\[\forall (p_1,p_2,\alpha)\in P^2\times[0,1],(\alpha p_1+(1-\alpha)p_2)\in P\]
C'est à dire que étant donné deux points \(p_1,p_2\) dans l'ensemble \(P\),
tout point appartenant au segment de droite reliant \(p_1\) et \(p_2\)
appartient également à \(P\).
\end{definition}

\begin{definition}
Les \emph{sommets} d'un ensemble convexe \(P\) est le sous-ensemble \(S\) de
\(P\) de tous les éléments ne pouvant pas être exprimés comme une combinaison
linéaire convexe d'autres éléments:
\[S=\{s\in P:\forall (p_1,p_2,\alpha)\in (P\setminus\{s\})^2\times[0,1],(\alpha
p_1+(1-\alpha)p_2)\neq s\}\] C'est à dire tous les points qui ne se situent pas
sur une droite reliant deux autres points de l'ensemble \(P\), quels que soient
ces derniers points.
\end{definition}

\begin{corollary}
Tout élément \(p\) d'un ensemble convexe \(P\) peut être formulé comme une
combinaison linéaire convexe de ses sommets.
\begin{proof}
\(p\) est soit un sommet, soit \(p\) n'est pas un sommet:
\begin{itemize}
	\item Si il est un sommet, alors il est effectivement la combinaison
	linéaire convexe triviale \(p\).
	\item Sinon, par la définition des sommets, il peut être exprimé par
	une combinaison linéaire convexe d'autres points de \(P\), par exemple ses
	sommets.
\end{itemize}
\end{proof}
\end{corollary}

\subsection{Résultats fondamentaux}
\begin{theorem}
\label{P is convex}
L'ensemble \(P = \{x:Ax\le b,x\ge 0\}\) est convexe.
\begin{proof}
Pour toute combinaison linéaire convexe de facteurs \((\alpha, 1-\alpha)\)
d' éléments \(x_1,x_2 \in P\), on a
\[A(\alpha x_1+(1-\alpha)x_2) = A\alpha x_1 + A(1-\alpha)x_2
\le \alpha b + (1-\alpha) b = b\]
\[\Rightarrow A(\alpha x_1+(1-\alpha)x_2) \le b\]
Pour la second contrainte, on a
\[\alpha x_1+(1-\alpha)x_2\ge 0\]
Car c'est un combinaison linéaire convexe d'éléments positifs.\\
On en conclut que toute combinaison linéaire convexe d'éléments de \(P\) a
répond également aux contraintes définissant l'ensemble \(P\).

\end{proof}
\end{theorem}

\begin{theorem}
L'ensemble \(P = \{x:Ax\le b,x\ge 0\}\) est
\begin{itemize}
	\item Soit vide;
	\item Soit un polyèdre convexe;
	\item Soit un ensemble polyédrique non-borné.
\end{itemize}
\end{theorem}

\begin{theorem}
\label{solutions contains a vertex}
Si \(P\) est un polyèdre convexe, alors l'ensemble des solutions optimales
du problème \(\text{Min }\{cx:x\in P\}\) contient au moins un sommet de \(P\).
\begin{proof}
Soient \(s^{(1)},\dots,s^{(k)}\) les sommets de \(P\) et
\(cs^{(m)}=\min_ics^{(i)}\).\\ Puisque \(P\) est convexe, chacun de ses points
peut être exprimé comme une combinaison linéaire convexe de ses sommets. Pour
toute solution \(x\) du problème, on a
\[\exists (\alpha_1,\dots,\alpha_k)\in[0,1]^k:x=\sum_i \alpha_is^{(i)}\text{
(et}\sum_i\alpha_i=1\text{)}\] En multipliant par \(c\), on a
\[cx=\sum_ic\alpha_is^{(i)}\ge cs^{(m)}\]
Donc, le sommet \(s^{(m)}\) ayant la plus petite évaluation parmis les autres
sommets est une des solution optimale.
\end{proof}
\end{theorem}

\begin{corollary}
L'ensemble des solutions optimales d'un problème contient au moins un sommet de
l'ensemble des solutions admissibles.
\begin{proof}
On peut le déduire directement du résultat \ref{P is convex} et du résultat
\ref{solutions contains a vertex}.
\end{proof}
\end{corollary}

\begin{notation}
\(P_j\) est la \(j\)\ieme \(\,\) colonne de la matrice \(A\).
On notera également \(P_0=b\).
On peut alors exprimer les contraintes d'un problème linéaire comme suit:
\[\sum_{j=1}^n x_jP_j \begin{Bmatrix}\le\\=\end{Bmatrix} P_0\]
\end{notation}

\begin{theorem}
Étant donné le problème linéaire \(\text{Min }\{cx:x\in P\}\) avec
\(P = \{x:Ax=b,x\ge 0\}\), si \(A\) est de rang \(m\), alors tout sommet de
\(P\) est une solution de base admissible.
\begin{proof}
Soit \(s=(s_1,\dots,s_k,0\dots,0)\) un sommet de \(P\) (c'est donc une solution
admissible), avec \(s_i\ge0\;\forall i\in\{1,\dots,k\}\). On peut toujours s'y ramener
en réordonnant les variables, de manière à avoir les zéros à la fin.\\
Montrons que \(P_1,\dots,P_k\) sont linéairement indépendants par l'absurde:\\
Par la notation précédente, on a
\[\sum_{j=1}^ks_jP_j=P_0=b\]
Si \(P_1,\dots,P_k\) ne sont pas linéairement indépendants, on a également
\[\exists (\alpha_1,\dots,\alpha_k)\in\mathbb{R}^k:\sum_{j=1}^k\alpha_jP_j=0\]
avec \(\alpha_1,\dots,\alpha_k\) non tous nuls.\\
En choisissant un nombre \(\epsilon\in\mathbb{R}\) tel que \(|\epsilon
\alpha_j|< s_j \;\forall j\in\{1,\dots,k\}\), on peut écrire:
\[\begin{dcases}
	\sum_{j=1}^k(s_j+\epsilon\alpha_j)P_j=P_0 \\
	\sum_{j=1}^k(s_j-\epsilon\alpha_j)P_j=P_0
\end{dcases}\]
On en déduit qu'il existe deux solutions admissibles
\[x^{(1)} = (s_1+\epsilon\alpha_1,\dots,s_k+\epsilon\alpha_k)\]
\[x^{(2)} = (s_1+\epsilon\alpha_1,\dots,s_k-\epsilon\alpha_k)\]
\begin{note}
On a imposé la condition \(|\epsilon \alpha_j|< s_j \;\forall j\in\{1,\dots,k\}\) afin
de garantir que \(x^{(1)}\ge0\) et \(x^{(2)}\ge0\).
\end{note}
On a donc
\[s=\frac{1}{2}(x^{(1)}+x^{(2)})\]
Ce qui est contraire à l'hypothèse de \(s\) étant un sommet. On en conclut que
\(P_1,\dots,P_k\) sont linéairement indépendants.\\
Puisque \(A\) est de rang \(m\), il existe au maximum \(m\) vecteurs colonnes
extraits de \(A\) étant linéairement indépendants entre eux. Donc \(k\le m\).
\begin{itemize}
	\item Si \(k=m\), alors \(s\) est la solution admissible de base associée à
	la base \(B=(P_1\dots P_k)\);
	\item Sinon, \(k<m\), et \(s\) est la solution admissible de base associée à
	la base \(B=(P_1\dots P_k\cdot P_{i_1}\dots P_{i_{m-k}})\)
	construite en choisissant \(m-k\) colonnes dans \(A\) de telle sorte que
	\(\det(B)\neq 0\). C'est toujours possible, puisque \(A\) est de rang \(m\).
\end{itemize}
\end{proof}
\end{theorem}

\begin{theorem}
Étant donné le problème linéaire \(\text{Min }\{cx:x\in P\}\) avec
\(P = \{x:Ax=b,x\ge 0\}\), si \(A\) est de rang \(m\), alors toute solution de
base admissible du problème est un sommet de \(P\) (réciproque du théorème
précédent).
\begin{proof}
Soit \(s=(s_1,\dots,s_m,0,\dots,0)\) une solution de base admissible. Cela
signifie que la matrice \(B=(P_1\dots P_m)\) est une base. Prouvons que \(s\)
est un sommet:
\paragraph{}
Si \(s_i=0\;\forall i\in[1,m]\), alors \(s\) ne peut pas être exprimé comme une
combinaison linéaire convexe de deux autres éléments de \(P\), et donc \(s\) est
un sommet.
\paragraph{}
Sinon, prouvons par l'absurde que \(s\) est aussi un sommet. Si \(s\) n'est pas
un sommet, alors
\[\exists (x^{(1)},x^{(2)},\alpha)\in P^2\times[0,1]:s=\alpha x^{(1)}+(1-\alpha)x^{(2)}\]
Et comme \(x^{(1)}\ge0\) et \(x^{(2)}\ge0\), on peut en déduire que
\[x^{(1)}_i=x^{(2)}_i=0\;\forall i\in[m+1,n]\]
On peut alors écrire
\[Bx^{(1)}=Bx^{(2)}=b\]
Et puisque \(B\) est carrée et inversible, on a \(x^{(1)}=x^{(2)}=s\).
Donc \(s\) est un sommet.
\end{proof}
\end{theorem}

\section{Algorithme du simplex}
\subsection{Intuition}
L'algorithme du simplex résout des problèmes de programmation linéaire continus,
et peut être facilement imaginé dans le cas d'un problème à deux variables
continues \((x,y)\). Représentons dans le plan chacune des contraintes comme
l'ensemble des points satisfaint cette contrainte. L'ensemble des solutions
admissibles (qui est l'intersection de toutes ces régions) est alors représenté
par un polygone.
\begin{center}\textbf{\textbf{}}
\includegraphics[width=\textwidth]{simplex-xy.png}
\end{center}
On a prouvé précédemment qu'une des solutions optimales se trouve sur l'un des
sommets du polygone. L'algorithme du simplex démarre sur l'un des sommets et
passe de sommet en sommet vers la solution optimale, toujours en améliorant la
solution courante.
\end{document}
