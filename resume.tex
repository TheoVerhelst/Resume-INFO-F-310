\documentclass[a4paper]{report}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[francais]{babel}
\usepackage[top=1.5cm, left=2.5cm, bottom=1.5cm, right=2.5cm]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{tikz}

\author{Théo Verhelst}
\title{Résumé du cours d'Algorithmique et Recherche Opérationelle\\
\emph{INFO-F310}}

\theoremstyle{definition}
\newtheorem*{definition}{Définition}
\newtheorem*{notation}{Notation}
\theoremstyle{remark}
\newtheorem*{note}{Note}
\theoremstyle{plain}
\newtheorem*{result}{Résultat}
\newtheorem*{corollary}{Corollaire}

\begin{document}
\maketitle
\tableofcontents

\chapter{Introduction}
\paragraph{}
Parmi les nombreux domaines compris dans l'algorithmique et la recherche
opérationelle, dans ce cours seront abordés la \emph{programmation mathématique}
et les \emph{méthodes combinatoires dans les graphes}.

\chapter{Programmation mathématique}
\section{Définition de la programmation mathématique}
\paragraph{}
La programmation mathématique est une modélisation de problèmes (qui peuvent
provenir d'une large gamme de domaines) ainsi que leur résolution.

\begin{definition}
Un problème de programmation mathématique est défini
par un tuple \((z, G)\), où
\[z:E^n\to E:(x_1,\dots,x_n)\mapsto f(x_1,\dots,x_n)\]
est appelée \emph{fonction économique} (ou encore \emph{fonction de coût}), et
où \[\star\in\{=,\ge,\le\},\;G=\{(g_j(x_1,\dots,x_n)\star b_j)\;\forall
j\in[1,m]\}\] sont appelées \emph{contraintes}.
\((x_1,\dots,x_n)\) sont les \emph{variables} du problème.
\end{definition}

\begin{notation}
On notera
\[g_j(x_1,\dots,x_n)\begin{Bmatrix}\le\\\ge\\=\end{Bmatrix}b_j\quad\forall
j\in[1,m]\]
\end{notation}

\begin{definition}
On classe les problèmes selon la nature de l'ensemble
\(E\):
\begin{itemize}
	\item \(E=\mathbb{R}\) correspond aux problèmes continus
	\item \(E=\mathbb{Z}\) correspond aux problèmes entiers
	\item \(E=\{1,0\}\) correspond aux problèmes booléens
\end{itemize}
Ces classes peuvent être mixées, si toutes les variables ou contraintes ne sont
pas définies dans le même ensemble.
\end{definition}

\begin{definition}
Résoudre un problème de programmation mathématique consiste à trouver les
valeurs \((x_1,\dots,x_n)\) qui maximisent ou minimisent le plus
possible d'une valeur donnée la fonction économique \(z\),
tout en satisfaisant toutes les contraintes \(g_j\).
\end{definition}

\begin{definition}
Pour une solution donnée \((x_1,\dots,x_n)\), on dit qu'une contrainte
d'indice \(j\) est saturée quand:
\[g_j(x_1,\dots,x_n)=b_j\]
Cette notion n'est intéressante que pour les contraintes à inégalités, et
représente le cas où une ressource est utilisée à son maximum.
\end{definition}

\begin{definition}
Quand les fonctions \(f\) et \(g_j\) sont linéaires en \(x_i\), alors le
problème est appelé problème de programmation linéaire.
\end{definition}

\section{Classification des problèmes de programmation linéaire}
\begin{tikzpicture}
	\node (n1) at (8,10) {Mixtes - entiers};
	\node (n2) at (6,8)  {Mixtes - booléens};
	\node (n3) at (8,6)  {Booléens};
	\node (n4) at (10,8) {Entiers};
	\node (n5) at (4,6)  {Continus};

	\foreach \from/\to in {n2/n1, n4/n1, n5/n2, n3/n2}
		\draw[->,thick] (\from) -- (\to);
	\draw[<->,thick] (n3) -- (n4);
\end{tikzpicture}

\paragraph{}On peut classer les problèmes de programmation linéaire selon la
nature de leur variable. Le sens des flèches dans le schéma indique qu'une
méthode permettant de résoudre le problème à la destination de la flèche permet
également de résoudre un problème à la base de la flèche. On peut donc en
conclure qu'un solveur de problème mixant variables entières et continues permet
de résoudre tout type de problème de programmation linéaire.
\begin{note}
Un problème à nombre entiers peut également être résolu par un
solveur booléens: on pourrait imaginer convertir tous les variables entières en
suites de variables booléennes grâce à la représentation binaire du nombre.
\end{note}

\section{Respect d'un nombre paramétrique de contraintes}
On peut étendre la définition de la programmation mathématique en permettant
de ne respecter qu'un nombre \(m'\) de contraintes, avec \(m'<m\).
Pour cela, introduisons \(m\) variables booléennes \(\delta_i\) qui indiqueront
si la contrainte \(i\) est respectée. Introduisons également un nombre \(M\),
qui est supérieur à toutes les valeurs que peuvent prendre les contraintes
\(g_i\). On peut alors réécrire les contraintes
comme suit:
\[g_i(x_1,\dots,x_n)-b_i\begin{Bmatrix}\le\\\ge\\=\end{Bmatrix}M(1-\delta_i)\]
et rajouter la contrainte suivante:
\[\sum_{i=1}^{m}\delta_i\ge m'\]
Le problème résultant reste un problème de programmation linéaire si \(f\) et
\(g\) sont des fonctions linéaires.

\section{Problèmes  de programmation linéaire continus}
\subsection{Forme matricielle}
On commencera par exprimer les problèmes de programmation linéaire continus sous
forme matricielle:
\[\begin{cases}
	\text{Min\;} cx & c\in\mathbb{R}^{1\times n},\, x\in\mathbb{R}^{n\times 1}_+
	\\
	Ax \le b        & A\in\mathbb{R}^{m\times n},\, b\in\mathbb{R}^{m\times 1}
	\\
	x \ge 0
\end{cases}\]
où \(x\) est le vecteur de variables, \(c\) est le vecteur de coefficients de
la fonction économique \(z\), \(A\) est la matrices de coefficients des
contraintes, et \(b\) est le vecteur de termes indépendants des contraintes.
Un certain nombre de restrictions sont imposées sur la formulation du problème,
car on peut toujours se ramener à ce problème plus restreint:
\begin{itemize}
	\item On se passe des contraintes en \(\ge\) et \(=\), car on peut
	toujours reformuler ces contraintes avec d'autres contraintes en \(\le\):
	\[a=b\Leftrightarrow a\le b\land -a\le-b\]
	\[a\ge b\Leftrightarrow -a\le-b\]

	\item On ne considère que les  problèmes où les variables
	\((x_1,\dots,x_n)\) sont positives, car ces variables
	représentent souvent des quantités, et ne peuvent donc pas être négatives.
	Si toutefois une variable \(x_i,\,i\in[1,n]\) peut être négative, on se
	ramène dans le cas positif en posant
	\[x_i=y_i-z_i\]
	où \(y_i,z_i\) sont deux nouvelles variables dans \(\mathbb{R}^+\).

	\item On ne considère que la minimisation de la fonction
	économique, car on peut ramener un problème de maximisation en un
	problème de minimisation en prenant l'opposé de la fonction économique.
\end{itemize}

\subsection{Variables d'écart}
Étant donné un problème continu linéaire
\[\begin{cases}
\text{Min\;} cx \\
Ax \le b \\
x \ge 0
\end{cases}\]
Afin de pouvoir résoudre le problème avec les outils de l'algèbre linéaire, on
introduit \(m\) nouvelles variables positives
\([t_1,\dots,t_m]=t\), et on reformule les contraintes comme suit:
\[Ax+t=b\]
Les variables \(t_j\) sont appelées variables d'écart, et représentent la
quantité de ressource qui est encore disponible pour une contrainte donnée.
On en déduit que quand \(t_j=0\), la contrainte \(j\) est saturée.
Par la suite, nous admettrons l'utilisation implicite de variables d'écart,
et considererons généralement les problèmes de la forme
\[\begin{cases}
\text{Min\;} cx \\
Ax = b \\
x \ge 0
\end{cases}\]

\begin{notation}
On notera le problème
\[\begin{cases}
\text{Min } cx \\
Ax \begin{Bmatrix}\le\\=\end{Bmatrix}b \\
x \ge 0
\end{cases}\]
comme suit:
\[\text{Min }\{cx:Ax\begin{Bmatrix}\le\\=\end{Bmatrix}b,x\ge0\}\]
\end{notation}

\subsection{Définitions}
\begin{definition}
Une \emph{solution} est une instance du vecteur \(x\) telle que \(Ax=b\).
\end{definition}

\begin{definition}
Une \emph{solution admissible} est une instance du
vecteur \(x\) telle que \(Ax=b\) et \(x \ge 0\).
\end{definition}

\begin{definition}
Une \emph{base} \(B\) est une matrice carrée \(m\times
m\) extraite de la matrice \(A\), avec \(\det(B)\ne 0\). On parlera
d'\emph{indices de base} (réciproquement \emph{hors base}) et de \emph{variables
de base} (réciproquement \emph{hors base}) quand ces indices ou variables sont
inclus dans la base \(B\). Les lignes et colonnes incluses dans \(B\) ne doivent
pas forcément être adjacentes dans \(A\).
\end{definition}

\begin{definition}
Une solution \(x\) est une \emph{solution de base}
associée à une base \(B\) si et seulement si les variables hors base sont
nulles.
\end{definition}

\begin{definition}
Une solution de base \(x\) est \emph{explicitée} si et
seulement si la base associée \(B\) est la matrice unité \(m\times m\).
\end{definition}

\begin{definition}
Une \emph{combinaison linéaire convexe} d'éléments \(p_1,\dots,p_n\) est
une expression de la forme \[\sum_{i=1}^n\alpha_ip_i\] où \(\alpha_i\in[0,1]
\;\forall i\in[1,n]\) et où \(\sum_{i=1}^n\alpha_i=1\)
\end{definition}

\begin{definition}
Un ensemble \(P\) est \emph{convexe} si est seulement si toute combinaison
linéaire convexe de ses éléments appartient également à \(P\):
\[\forall (p_1,p_2,\alpha)\in P^2\times[0,1],(\alpha p_1+(1-\alpha)p_2)\in P\]
C'est à dire que étant donné deux points \(p_1,p_2\) dans l'ensemble \(P\),
tout point appartenant au segment de droite reliant \(p_1\) et \(p_2\)
appartient également à \(P\).
\end{definition}

\begin{definition}
Les \emph{sommets} d'un ensemble convexe \(P\) est le sous-ensemble \(S\) de
\(P\) de tous les éléments ne pouvant pas être exprimés comme une combinaison
linéaire convexe d'autres éléments:
\[S=\{s\in P:\forall (p_1,p_2,\alpha)\in (P\setminus\{s\})^2\times[0,1],(\alpha
p_1+(1-\alpha)p_2)\neq s\}\] C'est à dire tous les points qui ne se situent pas
sur une droite reliant deux autres points de l'ensemble \(P\), quels que soient
ces derniers points.
\end{definition}

\begin{corollary}
Tout élément \(p\) d'un ensemble convexe \(P\) peut être formulé comme une
combinaison linéaire convexe de ses sommets.
\begin{proof}
\(p\) est soit un sommet, soit \(p\) n'est pas un sommet:
\begin{itemize}
	\item Si il est un sommet, alors il est effectivement la combinaison
	linéaire convexe trivialle \(p\).
	\item Sinon, par la définition des sommets, il peut être exprimé par
	une combinaison linéaire convexe d'autres points de \(P\), par exemple ses
	sommets.
\end{itemize}
\end{proof}
\end{corollary}

\subsection{Résultats fondamentaux}
\begin{result}
L'ensemble \(P = \{x:Ax\le b,x\ge 0\}\) est convexe.
\begin{proof}
Pour toute combinaison linéaire convexe de facteurs \((\alpha, 1-\alpha)\)
d' éléments \(x_1,x_2 \in P\), on a
\[A(\alpha x_1+(1-\alpha)x_2) = A\alpha x_1 + A(1-\alpha)x_2
\le \alpha b + (1-\alpha) b = b\]
\[\Rightarrow A\alpha x_1 + A(1-\alpha)x_2 \le b\]
Pour la second contrainte, on a
\[\alpha x_1+(1-\alpha)x_2\ge 0\]
Car c'est un combinaison linéaire convexe d'éléments positifs.\\
On en conclut que toute combinaison linéaire convexe d'éléments de \(P\) a
répond également aux contraintes définissant l'ensemble \(P\).

\end{proof}
\end{result}

\begin{result}
L'ensemble des solutions admissibles d'un problème linéaire est
\begin{itemize}
	\item Soit vide;
	\item Soit un polyèdre convexe;
	\item Soit un ensemble polyédrique non-borné.
\end{itemize}
\end{result}

\begin{result}
Si \(P\) est un polyèdre convexe, alors l'ensemble des solutions optimales
du problème
\[\begin{cases}
\text{Min }cx\\
x\in P
\end{cases}\]
contient au moins un sommet de \(P\).
\begin{proof}
Soient \(s_1,\dots,s_k\) les sommets de \(P\) et \(cs_m=\min_ics_i\).\\
Puisque \(P\) est convexe, chacun de ses points peut être exprimé comme une
combinaison de ses sommets. Pour toute solution \(x\) du problème, on a
\[x\in P\Rightarrow \exists \alpha\in[0,1]^k:x=\sum_i \alpha_is_i\text{
(et}\sum_i\alpha_i=1\text{)}\] En multipliant par \(c\), on a
\[cx=\sum_ic\alpha_is_i\ge cs_m\]
Donc, le sommet \(s_m\) ayant la plus petite évaluation parmis les autres
sommets est une des solution optimale.
\end{proof}
\end{result}

\begin{notation}
\(P_j\) est la \(j\)\ieme \(\,\) colonne de la matrice \(A\).
On notera également \(P_0=b\).
\end{notation}

\begin{result}
Si \(A\) est de rang \(m\), alors tout sommet de l'ensemble des solutions
admissibles est une solution de base admissible.
\begin{proof}
Soit \(s=(s_1,\dots,s_k,0\dots,0)\) une solution admissible, avec
\(s_i\ge0\;\forall i\in[1,k]\)\\
Si \(P_1,\dots,P_k\) ne sont pas linéairement indépendants, alors
\[\exists \alpha\in\mathbb{R}^k:\sum_{j=1}^k\alpha_jP_j=0\]
\end{proof}
\end{result}

\section{Algorithme du simplex}
\subsection{Intuition}
L'algorithme du simplex résout des problèmes de programmation linéaire continus,
et peut être facilement imaginé dans le cas d'un problème à deux variables
continues \((x,y)\). Représentons dans le plan chacune des contraintes comme
l'ensemble des points satisfaint cette contrainte. L'ensemble des solutions
admissibles (qui est l'intersection de toutes ces régions) est alors représenté
par un polygone.
\begin{center}\textbf{\textbf{}}
\includegraphics[width=\textwidth]{simplex-xy.png}
\end{center}
On peut prouver que la solution se trouve sur un des sommets du polygone (ce
sera fait plus tard). L'algorithme du simplex démarre sur l'un de ces sommets et
passe de sommet en sommet vers la solution optimale, toujours en améliorant la
solution courante.
\end{document}
